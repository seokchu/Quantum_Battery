{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "751b7697",
   "metadata": {},
   "source": [
    "# Negative Curriculum vs Positive Curriculum: Open Quantum Battery (Dicke Model) 비교 연구\n",
    "\n",
    "## 연구 목적\n",
    "양자 배터리 충전에서 **Negative Curriculum Learning**(제약 기반 조각적 학습)이\n",
    "**Positive Curriculum**(목표 지향적 학습) 및 기타 SOTA 방식보다 실제로 효과적인지를\n",
    "**객관적으로** 검증합니다.\n",
    "\n",
    "### 비교 방법론 (4가지)\n",
    "| 방법 | 레이블 | 핵심 아이디어 |\n",
    "|------|--------|--------------|\n",
    "| Negative Curriculum | **Ours** | \"함정에 빠지지 마라\" → 제약을 단계적으로 학습 |\n",
    "| Positive Curriculum | **PosCurr** | \"쉬운 목표부터 달성하라\" → 목표를 단계적으로 상향 |\n",
    "| No Curriculum | **Vanilla** | 커리큘럼 없는 표준 RL |\n",
    "| Reverse Curriculum | **RevCurr** | 목표 근처에서 출발하여 역방향으로 탐색 확장 (SOTA) |\n",
    "\n",
    "### 객관성 보장\n",
    "- 동일 SAC 에이전트 구조, 동일 하이퍼파라미터\n",
    "- 동일 random seed (3회 반복, 평균 ± std 보고)\n",
    "- 동일 총 학습 에피소드 수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a8b987",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b066ec04",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from copy import deepcopy\n",
    "from collections import deque\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import qutip as qt\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "\n",
    "# PyTorch device\n",
    "device = torch.device('cpu')\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"QuTiP version: {qt.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Matplotlib style\n",
    "plt.rcParams.update({\n",
    "    'figure.figsize': (12, 6),\n",
    "    'font.size': 12,\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f51807",
   "metadata": {},
   "source": [
    "## 2. Open Quantum Battery Environment (Dicke Model)\n",
    "\n",
    "### Hamiltonian\n",
    "$$H = \\omega_c a^\\dagger a + \\omega_0 J_z + \\frac{g(t)}{\\sqrt{N}}(a^\\dagger + a)(J_+ + J_-)$$\n",
    "\n",
    "### Lindblad Master Equation (OQB)\n",
    "$$\\dot{\\rho} = -i[H, \\rho] + \\kappa \\mathcal{D}[a]\\rho + \\gamma \\sum_i \\mathcal{D}[\\sigma_-^{(i)}]\\rho + \\gamma_\\phi \\sum_i \\mathcal{D}[\\sigma_z^{(i)}]\\rho$$\n",
    "\n",
    "- $N=4$ qubits (Dicke manifold: total spin $j=2$)\n",
    "- Cavity Fock space truncation: $n_{max}=8$\n",
    "- 제어 파라미터: $g(t) \\in [0, 2]$, $\\Delta(t) \\in [-1, 1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bc9270",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class DickeOQBEnv(gym.Env):\n",
    "    \"\"\"Open Quantum Battery environment based on the Dicke model.\n",
    "\n",
    "    N qubits collectively coupled to a single cavity mode with dissipation.\n",
    "    Uses the Dicke manifold (collective spin-j representation) for efficiency.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {'render_modes': []}\n",
    "\n",
    "    def __init__(self, N=4, n_cav=8, omega_c=1.0, omega_0=1.0,\n",
    "                 kappa=0.03, gamma=0.01, gamma_phi=0.005,\n",
    "                 dt=0.2, max_steps=30, g_max=2.0, delta_max=1.0,\n",
    "                 init_photons=4):\n",
    "        super().__init__()\n",
    "        self.N = N\n",
    "        self.j = N / 2.0  # Total spin\n",
    "        self.n_cav = n_cav\n",
    "        self.omega_c = omega_c\n",
    "        self.omega_0_base = omega_0\n",
    "        self.kappa = kappa\n",
    "        self.gamma = gamma\n",
    "        self.gamma_phi = gamma_phi\n",
    "        self.dt = dt\n",
    "        self.max_steps = max_steps\n",
    "        self.g_max = g_max\n",
    "        self.delta_max = delta_max\n",
    "        self.init_photons = init_photons\n",
    "\n",
    "        # Dimensions: cavity ⊗ spin\n",
    "        self.dim_cav = n_cav\n",
    "        self.dim_spin = int(2 * self.j + 1)  # 2j+1 states in Dicke manifold\n",
    "\n",
    "        # Cavity operators\n",
    "        a = qt.tensor(qt.destroy(self.dim_cav), qt.qeye(self.dim_spin))\n",
    "        self.a = a\n",
    "        self.a_dag = a.dag()\n",
    "        self.n_op = a.dag() * a  # photon number\n",
    "\n",
    "        # Collective spin operators (Dicke manifold)\n",
    "        Jz = qt.tensor(qt.qeye(self.dim_cav), qt.jmat(self.j, 'z'))\n",
    "        Jp = qt.tensor(qt.qeye(self.dim_cav), qt.jmat(self.j, '+'))\n",
    "        Jm = qt.tensor(qt.qeye(self.dim_cav), qt.jmat(self.j, '-'))\n",
    "        self.Jz = Jz\n",
    "        self.Jp = Jp\n",
    "        self.Jm = Jm\n",
    "\n",
    "        # Collapse operators for Lindblad\n",
    "        self.c_ops_base = []\n",
    "        # Cavity decay\n",
    "        if kappa > 0:\n",
    "            self.c_ops_base.append(np.sqrt(kappa) * a)\n",
    "        # Collective spin decay (approximation in Dicke manifold)\n",
    "        if gamma > 0:\n",
    "            self.c_ops_base.append(np.sqrt(gamma * N) * qt.tensor(\n",
    "                qt.qeye(self.dim_cav), qt.jmat(self.j, '-')))\n",
    "        # Collective dephasing\n",
    "        if gamma_phi > 0:\n",
    "            self.c_ops_base.append(np.sqrt(gamma_phi * N) * Jz)\n",
    "\n",
    "        # Battery Hamiltonian (for ergotropy calculation)\n",
    "        # H_B = omega_0 * J_z (battery = spin system)\n",
    "        self.H_B = self.omega_0_base * Jz\n",
    "\n",
    "        # Ground state of battery\n",
    "        self.ground_state_spin = qt.tensor(\n",
    "            qt.fock(self.dim_cav, 0),\n",
    "            qt.spin_state(self.j, -self.j))  # |0_cav, -j>\n",
    "\n",
    "        # Maximum energy of battery\n",
    "        self.max_energy = self.omega_0_base * self.N  # from -j to +j\n",
    "\n",
    "        # Observation and action spaces\n",
    "        # State: [energy_stored, purity, entropy, <n_cav>, <Jz>/j]\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=np.array([0.0, 0.0, 0.0, 0.0, -1.0], dtype=np.float32),\n",
    "            high=np.array([1.0, 1.0, 5.0, float(n_cav), 1.0], dtype=np.float32)\n",
    "        )\n",
    "        # Action: [g(t), delta(t)]\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([-1.0, -1.0], dtype=np.float32),\n",
    "            high=np.array([1.0, 1.0], dtype=np.float32)\n",
    "        )\n",
    "\n",
    "        self.rho = None\n",
    "        self.step_count = 0\n",
    "        self.prev_ergotropy = 0.0\n",
    "\n",
    "    def _build_hamiltonian(self, g, delta):\n",
    "        \"\"\"Build the Dicke Hamiltonian with given coupling and detuning.\"\"\"\n",
    "        omega_0 = self.omega_0_base + delta\n",
    "        H = (self.omega_c * self.n_op\n",
    "             + omega_0 * self.Jz\n",
    "             + (g / np.sqrt(self.N)) * (self.a_dag + self.a) * (self.Jp + self.Jm))\n",
    "        return H\n",
    "\n",
    "    def _compute_battery_state(self):\n",
    "        \"\"\"Compute the reduced density matrix of the battery (spin system).\"\"\"\n",
    "        # Trace out cavity\n",
    "        rho_full = self.rho\n",
    "        rho_battery = rho_full.ptrace(1)  # trace out cavity (index 0)\n",
    "        return rho_battery\n",
    "\n",
    "    def _compute_ergotropy(self, rho_B):\n",
    "        \"\"\"Compute ergotropy: W = Tr(rho H_B) - sum_i eps_i r_i\n",
    "        where r_i are eigenvalues of rho sorted descending,\n",
    "        and eps_i are eigenvalues of H_B sorted ascending.\n",
    "        \"\"\"\n",
    "        H_B_local = self.omega_0_base * qt.jmat(self.j, 'z')\n",
    "        # Energy\n",
    "        energy = qt.expect(H_B_local, rho_B)\n",
    "        # Passive state energy\n",
    "        evals_rho = np.sort(np.real(rho_B.eigenenergies()))[::-1]  # descending\n",
    "        evals_H = np.sort(np.real(H_B_local.eigenenergies()))       # ascending\n",
    "        passive_energy = np.sum(evals_rho * evals_H)\n",
    "        ergotropy = max(0.0, energy - passive_energy)\n",
    "        return ergotropy\n",
    "\n",
    "    def _compute_obs(self, rho_B):\n",
    "        \"\"\"Compute observation vector from battery state.\"\"\"\n",
    "        H_B_local = self.omega_0_base * qt.jmat(self.j, 'z')\n",
    "        energy = qt.expect(H_B_local, rho_B)\n",
    "        # Normalize energy: shift so ground = 0, max = max_energy\n",
    "        energy_stored = (energy + self.omega_0_base * self.j) / self.max_energy\n",
    "        energy_stored = np.clip(energy_stored, 0.0, 1.0)\n",
    "\n",
    "        purity = np.real(qt.expect(rho_B * rho_B, qt.qeye(self.dim_spin)))\n",
    "        purity = np.clip(purity, 0.0, 1.0)\n",
    "\n",
    "        entropy = float(qt.entropy_vn(rho_B, 2))\n",
    "        entropy = max(0.0, entropy)\n",
    "\n",
    "        n_cav = float(np.real(qt.expect(self.n_op, self.rho)))\n",
    "        jz_expect = float(np.real(qt.expect(self.Jz, self.rho))) / self.j\n",
    "\n",
    "        obs = np.array([energy_stored, purity, entropy, n_cav, jz_expect],\n",
    "                       dtype=np.float32)\n",
    "        return obs\n",
    "\n",
    "    def reset(self, seed=None, options=None, init_state=None):\n",
    "        \"\"\"Reset environment. init_state can override for reverse curriculum.\"\"\"\n",
    "        super().reset(seed=seed)\n",
    "        if init_state is not None:\n",
    "            self.rho = init_state\n",
    "        else:\n",
    "            # Coherent state in cavity ⊗ ground spin: |alpha> ⊗ |j, -j>\n",
    "            # This provides photons as an energy source for charging\n",
    "            psi0 = qt.tensor(qt.coherent(self.dim_cav, np.sqrt(self.init_photons)),\n",
    "                             qt.spin_state(self.j, -self.j))\n",
    "            self.rho = qt.ket2dm(psi0)\n",
    "\n",
    "        self.step_count = 0\n",
    "        rho_B = self._compute_battery_state()\n",
    "        self.prev_ergotropy = self._compute_ergotropy(rho_B)\n",
    "        obs = self._compute_obs(rho_B)\n",
    "        return obs, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Execute one time step of the charging protocol.\"\"\"\n",
    "        # Map actions from [-1,1] to physical range\n",
    "        g = float((action[0] + 1.0) / 2.0 * self.g_max)      # [0, g_max]\n",
    "        delta = float(action[1] * self.delta_max)               # [-delta_max, delta_max]\n",
    "\n",
    "        # Build Hamiltonian and evolve\n",
    "        H = self._build_hamiltonian(g, delta)\n",
    "        result = qt.mesolve(H, self.rho, [0, self.dt], c_ops=self.c_ops_base)\n",
    "        self.rho = result.states[-1]\n",
    "\n",
    "        # Ensure valid density matrix\n",
    "        self.rho = (self.rho + self.rho.dag()) / 2.0\n",
    "        tr = np.real(self.rho.tr())\n",
    "        if abs(tr - 1.0) > 1e-6:\n",
    "            self.rho = self.rho / tr\n",
    "\n",
    "        self.step_count += 1\n",
    "\n",
    "        # Compute observables\n",
    "        rho_B = self._compute_battery_state()\n",
    "        ergotropy = self._compute_ergotropy(rho_B)\n",
    "        delta_erg = ergotropy - self.prev_ergotropy\n",
    "\n",
    "        obs = self._compute_obs(rho_B)\n",
    "        energy_stored = obs[0]\n",
    "        purity = obs[1]\n",
    "        entropy = obs[2]\n",
    "\n",
    "        # Base reward: ergotropy change (amplified) + absolute ergotropy bonus\n",
    "        reward = 10.0 * delta_erg / self.max_energy + 0.5 * ergotropy / self.max_energy\n",
    "\n",
    "        self.prev_ergotropy = ergotropy\n",
    "\n",
    "        terminated = self.step_count >= self.max_steps\n",
    "        truncated = False\n",
    "\n",
    "        info = {\n",
    "            'ergotropy': ergotropy,\n",
    "            'energy_stored': energy_stored,\n",
    "            'purity': purity,\n",
    "            'entropy': entropy,\n",
    "            'delta_ergotropy': delta_erg,\n",
    "            'g': g,\n",
    "            'delta': delta,\n",
    "        }\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "\n",
    "# Quick sanity check\n",
    "print(\"=== Environment Sanity Check ===\")\n",
    "env = DickeOQBEnv()\n",
    "obs, _ = env.reset(seed=42)\n",
    "print(f\"Initial obs: {obs}\")\n",
    "print(f\"Obs space: {env.observation_space}\")\n",
    "print(f\"Act space: {env.action_space}\")\n",
    "\n",
    "total_r = 0\n",
    "for i in range(5):\n",
    "    action = env.action_space.sample()\n",
    "    obs, r, done, trunc, info = env.step(action)\n",
    "    total_r += r\n",
    "    if i < 3:\n",
    "        print(f\"  Step {i+1}: erg={info['ergotropy']:.4f}, \"\n",
    "              f\"energy={info['energy_stored']:.4f}, \"\n",
    "              f\"purity={info['purity']:.4f}, r={r:.4f}\")\n",
    "print(f\"Env OK. Max battery energy = {env.max_energy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e751d460",
   "metadata": {},
   "source": [
    "## 3. SAC Agent (PyTorch)\n",
    "\n",
    "모든 커리큘럼 전략에서 동일한 Soft Actor-Critic 구조를 사용합니다.\n",
    "- **Actor**: Gaussian policy, 2-layer MLP (64-64)\n",
    "- **Critic**: Twin Q-networks\n",
    "- **Automatic entropy tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b85ba9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ---- Replay Buffer ----\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=50000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (np.array(states), np.array(actions), np.array(rewards, dtype=np.float32),\n",
    "                np.array(next_states), np.array(dones, dtype=np.float32))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# ---- Networks ----\n",
    "class GaussianActor(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "        )\n",
    "        self.mean_head = nn.Linear(hidden, act_dim)\n",
    "        self.log_std_head = nn.Linear(hidden, act_dim)\n",
    "        self.LOG_STD_MIN, self.LOG_STD_MAX = -20, 2\n",
    "\n",
    "    def forward(self, obs):\n",
    "        x = self.net(obs)\n",
    "        mean = self.mean_head(x)\n",
    "        log_std = self.log_std_head(x).clamp(self.LOG_STD_MIN, self.LOG_STD_MAX)\n",
    "        return mean, log_std\n",
    "\n",
    "    def sample(self, obs):\n",
    "        mean, log_std = self.forward(obs)\n",
    "        std = log_std.exp()\n",
    "        dist = Normal(mean, std)\n",
    "        x_t = dist.rsample()\n",
    "        action = torch.tanh(x_t)\n",
    "        log_prob = dist.log_prob(x_t) - torch.log(1 - action.pow(2) + 1e-6)\n",
    "        log_prob = log_prob.sum(-1, keepdim=True)\n",
    "        return action, log_prob\n",
    "\n",
    "    def get_action(self, obs_np):\n",
    "        with torch.no_grad():\n",
    "            obs_t = torch.FloatTensor(obs_np).unsqueeze(0).to(device)\n",
    "            action, _ = self.sample(obs_t)\n",
    "            return action.cpu().numpy().flatten()\n",
    "\n",
    "\n",
    "class TwinQCritic(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden=64):\n",
    "        super().__init__()\n",
    "        self.q1 = nn.Sequential(\n",
    "            nn.Linear(obs_dim + act_dim, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, 1))\n",
    "        self.q2 = nn.Sequential(\n",
    "            nn.Linear(obs_dim + act_dim, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden), nn.ReLU(),\n",
    "            nn.Linear(hidden, 1))\n",
    "\n",
    "    def forward(self, obs, act):\n",
    "        x = torch.cat([obs, act], dim=-1)\n",
    "        return self.q1(x), self.q2(x)\n",
    "\n",
    "\n",
    "# ---- SAC Agent ----\n",
    "class SACAgent:\n",
    "    def __init__(self, obs_dim, act_dim, lr=3e-4, gamma=0.99, tau=0.005,\n",
    "                 alpha_lr=3e-4, buffer_size=50000, batch_size=128):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.actor = GaussianActor(obs_dim, act_dim).to(device)\n",
    "        self.critic = TwinQCritic(obs_dim, act_dim).to(device)\n",
    "        self.critic_target = deepcopy(self.critic).to(device)\n",
    "\n",
    "        self.actor_optim = optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        self.critic_optim = optim.Adam(self.critic.parameters(), lr=lr)\n",
    "\n",
    "        # Auto entropy tuning\n",
    "        self.target_entropy = -act_dim\n",
    "        self.log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "        self.alpha_optim = optim.Adam([self.log_alpha], lr=alpha_lr)\n",
    "\n",
    "        self.buffer = ReplayBuffer(buffer_size)\n",
    "\n",
    "    @property\n",
    "    def alpha(self):\n",
    "        return self.log_alpha.exp().item()\n",
    "\n",
    "    def select_action(self, obs, deterministic=False):\n",
    "        if deterministic:\n",
    "            with torch.no_grad():\n",
    "                obs_t = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "                mean, _ = self.actor(obs_t)\n",
    "                return torch.tanh(mean).cpu().numpy().flatten()\n",
    "        return self.actor.get_action(obs)\n",
    "\n",
    "    def update(self):\n",
    "        if len(self.buffer) < self.batch_size:\n",
    "            return {}\n",
    "\n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(self.batch_size)\n",
    "        s = torch.FloatTensor(states).to(device)\n",
    "        a = torch.FloatTensor(actions).to(device)\n",
    "        r = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
    "        ns = torch.FloatTensor(next_states).to(device)\n",
    "        d = torch.FloatTensor(dones).unsqueeze(1).to(device)\n",
    "\n",
    "        alpha = self.log_alpha.exp().detach()\n",
    "\n",
    "        # Critic update\n",
    "        with torch.no_grad():\n",
    "            na, nlp = self.actor.sample(ns)\n",
    "            tq1, tq2 = self.critic_target(ns, na)\n",
    "            tq = torch.min(tq1, tq2) - alpha * nlp\n",
    "            target = r + (1 - d) * self.gamma * tq\n",
    "\n",
    "        q1, q2 = self.critic(s, a)\n",
    "        critic_loss = F.mse_loss(q1, target) + F.mse_loss(q2, target)\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "        # Actor update\n",
    "        new_a, new_lp = self.actor.sample(s)\n",
    "        q1_new, q2_new = self.critic(s, new_a)\n",
    "        q_new = torch.min(q1_new, q2_new)\n",
    "        actor_loss = (alpha * new_lp - q_new).mean()\n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        # Alpha update\n",
    "        alpha_loss = -(self.log_alpha * (new_lp.detach() + self.target_entropy)).mean()\n",
    "        self.alpha_optim.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.alpha_optim.step()\n",
    "\n",
    "        # Soft target update\n",
    "        for p, tp in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "            tp.data.copy_(self.tau * p.data + (1 - self.tau) * tp.data)\n",
    "\n",
    "        return {'critic_loss': critic_loss.item(), 'actor_loss': actor_loss.item()}\n",
    "\n",
    "    def save(self, path):\n",
    "        torch.save({'actor': self.actor.state_dict(),\n",
    "                     'critic': self.critic.state_dict()}, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        ckpt = torch.load(path, map_location=device)\n",
    "        self.actor.load_state_dict(ckpt['actor'])\n",
    "        self.critic.load_state_dict(ckpt['critic'])\n",
    "\n",
    "\n",
    "print(\"SAC Agent class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5f7a06",
   "metadata": {},
   "source": [
    "## 4. Curriculum Strategies\n",
    "\n",
    "모든 전략은 동일 에피소드 수(총 2400)를 3개 스테이지로 분할합니다.\n",
    "\n",
    "### 4.1 Negative Curriculum (Ours)\n",
    "- **Stage 1**: 에너지 붕괴 페널티 (collapse 방지)\n",
    "- **Stage 2**: 엔트로피 증가 페널티 (순도 유지)\n",
    "- **Stage 3**: Vanilla reward (자유 탐색)\n",
    "\n",
    "### 4.2 Positive Curriculum (PosCurr)\n",
    "- **Stage 1**: 쉬운 목표 (ergotropy > 30% of max → bonus)\n",
    "- **Stage 2**: 중간 목표 (ergotropy > 60% of max → bonus)\n",
    "- **Stage 3**: 완충 목표 (ergotropy > 90% of max → bonus)\n",
    "\n",
    "### 4.3 Vanilla (No Curriculum)\n",
    "- 모든 스테이지에서 동일한 기본 reward\n",
    "\n",
    "### 4.4 Reverse Curriculum (RevCurr)\n",
    "- **Stage 1**: 완충 근처에서 시작 (작은 perturbation)\n",
    "- **Stage 2**: 중간 에너지 상태에서 시작\n",
    "- **Stage 3**: 완전 방전 상태(ground state)에서 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac88ca72",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class CurriculumManager:\n",
    "    \"\"\"Manages curriculum stages for different strategies.\"\"\"\n",
    "\n",
    "    def __init__(self, strategy, total_episodes, stages=3, env_params=None):\n",
    "        self.strategy = strategy\n",
    "        self.total_episodes = total_episodes\n",
    "        self.stages = stages\n",
    "        self.stage_size = total_episodes // stages\n",
    "        self.env_params = env_params or {}\n",
    "\n",
    "        # Negative curriculum penalty coefficients (moderate)\n",
    "        self.alpha_collapse = 2.0   # energy collapse penalty\n",
    "        self.beta_entropy = 1.5     # entropy increase penalty\n",
    "\n",
    "        # Positive curriculum target thresholds\n",
    "        self.pos_targets = [0.15, 0.35, 0.60]\n",
    "        self.pos_bonus = 1.0\n",
    "\n",
    "    def get_stage(self, episode):\n",
    "        return min(episode // self.stage_size, self.stages - 1)\n",
    "\n",
    "    def get_init_state(self, episode, env):\n",
    "        \"\"\"Get initial state for reverse curriculum.\"\"\"\n",
    "        stage = self.get_stage(episode)\n",
    "        if self.strategy != 'RevCurr':\n",
    "            return None\n",
    "\n",
    "        j = env.j\n",
    "        dim_cav = env.dim_cav\n",
    "        dim_spin = env.dim_spin\n",
    "\n",
    "        if stage == 0:\n",
    "            # Near fully charged: coherent cavity + mostly excited spins\n",
    "            alpha_cav = np.sqrt(1.0)  # few photons remaining\n",
    "            alpha = 0.85 + 0.15 * np.random.rand()\n",
    "            psi_spin = (alpha * qt.spin_state(j, j)\n",
    "                        + np.sqrt(1 - alpha**2) * qt.spin_state(j, j-1)).unit()\n",
    "            psi = qt.tensor(qt.coherent(dim_cav, alpha_cav), psi_spin)\n",
    "            return qt.ket2dm(psi)\n",
    "        elif stage == 1:\n",
    "            # Mid-energy state: some cavity photons + mid spin\n",
    "            alpha_cav = np.sqrt(2.0)\n",
    "            alpha = 0.5 + 0.3 * np.random.rand()\n",
    "            m_val = 0  # middle Jz eigenvalue\n",
    "            psi_spin = (alpha * qt.spin_state(j, m_val)\n",
    "                        + np.sqrt(1 - alpha**2) * qt.spin_state(j, m_val + 1)).unit()\n",
    "            psi = qt.tensor(qt.coherent(dim_cav, alpha_cav), psi_spin)\n",
    "            return qt.ket2dm(psi)\n",
    "        else:\n",
    "            return None  # default reset (coherent cavity + ground spin)\n",
    "\n",
    "    def shape_reward(self, episode, base_reward, info):\n",
    "        \"\"\"Apply curriculum-specific reward shaping.\"\"\"\n",
    "        stage = self.get_stage(episode)\n",
    "        max_erg = 4.0  # max_energy for N=4\n",
    "\n",
    "        if self.strategy == 'Ours':  # Negative Curriculum\n",
    "            if stage == 0:\n",
    "                # Stage 1: Penalize collapse, but keep base reward (exploration)\n",
    "                energy_drop = max(0.0, -info['delta_ergotropy'])\n",
    "                penalty = self.alpha_collapse * energy_drop / max_erg\n",
    "                return base_reward - penalty\n",
    "            elif stage == 1:\n",
    "                # Stage 2: Penalize decoherence (entropy > threshold)\n",
    "                entropy_penalty = self.beta_entropy * max(0.0, info['entropy'] - 0.3)\n",
    "                return base_reward - entropy_penalty\n",
    "            else:\n",
    "                # Stage 3: Pure exploration with base reward\n",
    "                return base_reward\n",
    "\n",
    "        elif self.strategy == 'PosCurr':  # Positive Curriculum\n",
    "            target = self.pos_targets[stage]\n",
    "            if info['ergotropy'] >= target * max_erg:\n",
    "                return base_reward + self.pos_bonus * (info['ergotropy'] / max_erg)\n",
    "            return base_reward\n",
    "\n",
    "        elif self.strategy == 'Vanilla':\n",
    "            return base_reward\n",
    "\n",
    "        elif self.strategy == 'RevCurr':\n",
    "            # Same reward, but initial state changes\n",
    "            return base_reward\n",
    "\n",
    "        return base_reward\n",
    "\n",
    "\n",
    "print(\"CurriculumManager class defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed195f9",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2a539f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(strategy, total_episodes=2400, max_steps=30, seed=42, verbose=True):\n",
    "    \"\"\"Train a SAC agent with the given curriculum strategy.\n",
    "\n",
    "    Returns:\n",
    "        dict with training history and final agent\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "\n",
    "    env = DickeOQBEnv(max_steps=max_steps)\n",
    "    obs_dim = env.observation_space.shape[0]\n",
    "    act_dim = env.action_space.shape[0]\n",
    "    agent = SACAgent(obs_dim, act_dim)\n",
    "    curriculum = CurriculumManager(strategy, total_episodes)\n",
    "\n",
    "    history = {\n",
    "        'episode_rewards': [],\n",
    "        'episode_ergotropy': [],\n",
    "        'episode_purity': [],\n",
    "        'episode_entropy': [],\n",
    "        'episode_energy': [],\n",
    "        'best_ergotropy': 0.0,\n",
    "        'convergence_episode': None,\n",
    "    }\n",
    "\n",
    "    convergence_threshold = 0.3 * env.max_energy  # 30% threshold for convergence\n",
    "    warmup_steps = 200  # Fill buffer before training\n",
    "\n",
    "    for ep in range(total_episodes):\n",
    "        init_state = curriculum.get_init_state(ep, env)\n",
    "        obs, _ = env.reset(seed=seed + ep, init_state=init_state)\n",
    "\n",
    "        ep_reward = 0.0\n",
    "        ep_ergotropy = 0.0\n",
    "        ep_purity = 0.0\n",
    "        ep_entropy = 0.0\n",
    "        ep_energy = 0.0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            if len(agent.buffer) < warmup_steps:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = agent.select_action(obs)\n",
    "\n",
    "            next_obs, base_reward, terminated, truncated, info = env.step(action)\n",
    "            reward = curriculum.shape_reward(ep, base_reward, info)\n",
    "\n",
    "            agent.buffer.push(obs, action, reward, next_obs, terminated or truncated)\n",
    "            obs = next_obs\n",
    "\n",
    "            if len(agent.buffer) >= warmup_steps:\n",
    "                agent.update()\n",
    "\n",
    "            ep_reward += reward\n",
    "            ep_ergotropy = info['ergotropy']\n",
    "            ep_purity = info['purity']\n",
    "            ep_entropy = info['entropy']\n",
    "            ep_energy = info['energy_stored']\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        history['episode_rewards'].append(ep_reward)\n",
    "        history['episode_ergotropy'].append(ep_ergotropy)\n",
    "        history['episode_purity'].append(ep_purity)\n",
    "        history['episode_entropy'].append(ep_entropy)\n",
    "        history['episode_energy'].append(ep_energy)\n",
    "\n",
    "        if ep_ergotropy > history['best_ergotropy']:\n",
    "            history['best_ergotropy'] = ep_ergotropy\n",
    "\n",
    "        if (history['convergence_episode'] is None\n",
    "                and ep_ergotropy >= convergence_threshold):\n",
    "            history['convergence_episode'] = ep\n",
    "\n",
    "        if verbose and (ep + 1) % (total_episodes // 6) == 0:\n",
    "            stage = curriculum.get_stage(ep)\n",
    "            print(f\"  [{strategy}] Ep {ep+1}/{total_episodes} | \"\n",
    "                  f\"Stage {stage+1} | Erg={ep_ergotropy:.3f} | \"\n",
    "                  f\"Pur={ep_purity:.3f} | R={ep_reward:.3f}\")\n",
    "\n",
    "    history['agent'] = agent\n",
    "    return history\n",
    "\n",
    "\n",
    "print(\"Training function defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cce42f",
   "metadata": {},
   "source": [
    "## 5.1 Run Experiments (3 Seeds × 4 Strategies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa7b821",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "STRATEGIES = ['Ours', 'PosCurr', 'Vanilla', 'RevCurr']\n",
    "STRATEGY_LABELS = {\n",
    "    'Ours': 'Negative Curriculum (Ours)',\n",
    "    'PosCurr': 'Positive Curriculum',\n",
    "    'Vanilla': 'Vanilla SAC (No Curriculum)',\n",
    "    'RevCurr': 'Reverse Curriculum (SOTA)'\n",
    "}\n",
    "STRATEGY_COLORS = {\n",
    "    'Ours': '#E63946',\n",
    "    'PosCurr': '#457B9D',\n",
    "    'Vanilla': '#2A9D8F',\n",
    "    'RevCurr': '#E9C46A'\n",
    "}\n",
    "\n",
    "SEEDS = [42, 123, 777]\n",
    "TOTAL_EPISODES = 1500\n",
    "MAX_STEPS = 30\n",
    "\n",
    "all_results = {}  # strategy -> list of history dicts\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\" Starting Experiments: Neg vs Pos Curriculum on OQB (Dicke)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for strategy in STRATEGIES:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\" Strategy: {STRATEGY_LABELS[strategy]}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    all_results[strategy] = []\n",
    "    for i, seed in enumerate(SEEDS):\n",
    "        print(f\"\\n--- Seed {seed} (Run {i+1}/{len(SEEDS)}) ---\")\n",
    "        hist = train_agent(strategy, total_episodes=TOTAL_EPISODES,\n",
    "                           max_steps=MAX_STEPS, seed=seed)\n",
    "        all_results[strategy].append(hist)\n",
    "        print(f\"  Best ergotropy: {hist['best_ergotropy']:.4f}\")\n",
    "        conv = hist['convergence_episode']\n",
    "        print(f\"  Convergence ep: {conv if conv else 'N/A'}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" All experiments completed!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924b9943",
   "metadata": {},
   "source": [
    "## 6. Results & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2920fb44",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def smooth(data, window=50):\n",
    "    \"\"\"Moving average smoothing.\"\"\"\n",
    "    if len(data) < window:\n",
    "        return data\n",
    "    kernel = np.ones(window) / window\n",
    "    return np.convolve(data, kernel, mode='valid')\n",
    "\n",
    "\n",
    "def compute_stats(results_list, key):\n",
    "    \"\"\"Compute mean and std across seeds for a given metric.\"\"\"\n",
    "    all_data = [np.array(r[key]) for r in results_list]\n",
    "    min_len = min(len(d) for d in all_data)\n",
    "    all_data = [d[:min_len] for d in all_data]\n",
    "    arr = np.array(all_data)\n",
    "    return arr.mean(axis=0), arr.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7611a0",
   "metadata": {},
   "source": [
    "### 6.1 학습 곡선: Episode vs Ergotropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d37f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Left: Raw learning curves\n",
    "ax = axes[0]\n",
    "for strategy in STRATEGIES:\n",
    "    mean, std = compute_stats(all_results[strategy], 'episode_ergotropy')\n",
    "    mean_s = smooth(mean, 50)\n",
    "    x = np.arange(len(mean_s))\n",
    "    ax.plot(x, mean_s, color=STRATEGY_COLORS[strategy],\n",
    "            label=STRATEGY_LABELS[strategy], linewidth=2)\n",
    "    std_s = smooth(std, 50)[:len(mean_s)]\n",
    "    ax.fill_between(x, mean_s - std_s, mean_s + std_s,\n",
    "                    color=STRATEGY_COLORS[strategy], alpha=0.15)\n",
    "\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Ergotropy')\n",
    "ax.set_title('Learning Curves: Episode Ergotropy (smoothed)')\n",
    "ax.legend(fontsize=9)\n",
    "ax.axhline(y=4.0, color='gray', linestyle='--', alpha=0.5, label='Max Energy')\n",
    "\n",
    "# Right: Episode rewards\n",
    "ax = axes[1]\n",
    "for strategy in STRATEGIES:\n",
    "    mean, std = compute_stats(all_results[strategy], 'episode_rewards')\n",
    "    mean_s = smooth(mean, 50)\n",
    "    x = np.arange(len(mean_s))\n",
    "    ax.plot(x, mean_s, color=STRATEGY_COLORS[strategy],\n",
    "            label=STRATEGY_LABELS[strategy], linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Episode Reward')\n",
    "ax.set_title('Learning Curves: Episode Reward (smoothed)')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig1_learning_curves.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Figure 1 saved: fig1_learning_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cc499a",
   "metadata": {},
   "source": [
    "### 6.2 최종 에르고트로피 분포 (Box Plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfbdeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "box_data = []\n",
    "labels = []\n",
    "colors = []\n",
    "for strategy in STRATEGIES:\n",
    "    final_ergs = [r['episode_ergotropy'][-1] for r in all_results[strategy]]\n",
    "    # Also get last 100 episodes average per seed\n",
    "    last_100 = [np.mean(r['episode_ergotropy'][-100:]) for r in all_results[strategy]]\n",
    "    box_data.append(last_100)\n",
    "    labels.append(STRATEGY_LABELS[strategy].replace(' (', '\\n('))\n",
    "    colors.append(STRATEGY_COLORS[strategy])\n",
    "\n",
    "bp = ax.boxplot(box_data, labels=labels, patch_artist=True, widths=0.5)\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.6)\n",
    "\n",
    "# Scatter individual points\n",
    "for i, (data, color) in enumerate(zip(box_data, colors)):\n",
    "    x = np.random.normal(i + 1, 0.04, len(data))\n",
    "    ax.scatter(x, data, color=color, zorder=5, s=60, edgecolors='black', linewidth=0.5)\n",
    "\n",
    "ax.set_ylabel('Final Ergotropy (last 100 eps avg)')\n",
    "ax.set_title('Final Charging Performance Comparison')\n",
    "ax.axhline(y=4.0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.text(0.5, 4.1, 'Max Energy = 4.0', fontsize=9, color='gray')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig2_boxplot.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Figure 2 saved: fig2_boxplot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c445cba",
   "metadata": {},
   "source": [
    "### 6.3 순도(Purity) 궤적 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b1ed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Purity over episodes\n",
    "ax = axes[0]\n",
    "for strategy in STRATEGIES:\n",
    "    mean, std = compute_stats(all_results[strategy], 'episode_purity')\n",
    "    mean_s = smooth(mean, 50)\n",
    "    x = np.arange(len(mean_s))\n",
    "    ax.plot(x, mean_s, color=STRATEGY_COLORS[strategy],\n",
    "            label=STRATEGY_LABELS[strategy], linewidth=2)\n",
    "    std_s = smooth(std, 50)[:len(mean_s)]\n",
    "    ax.fill_between(x, mean_s - std_s, mean_s + std_s,\n",
    "                    color=STRATEGY_COLORS[strategy], alpha=0.15)\n",
    "\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Purity')\n",
    "ax.set_title('State Purity Over Training')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# Entropy over episodes\n",
    "ax = axes[1]\n",
    "for strategy in STRATEGIES:\n",
    "    mean, std = compute_stats(all_results[strategy], 'episode_entropy')\n",
    "    mean_s = smooth(mean, 50)\n",
    "    x = np.arange(len(mean_s))\n",
    "    ax.plot(x, mean_s, color=STRATEGY_COLORS[strategy],\n",
    "            label=STRATEGY_LABELS[strategy], linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Von Neumann Entropy')\n",
    "ax.set_title('Entropy Over Training')\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig3_purity_entropy.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Figure 3 saved: fig3_purity_entropy.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bfd4f8",
   "metadata": {},
   "source": [
    "### 6.4 수렴 속도 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fbe13d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "conv_data = {}\n",
    "for strategy in STRATEGIES:\n",
    "    episodes = []\n",
    "    for r in all_results[strategy]:\n",
    "        ep = r['convergence_episode']\n",
    "        if ep is not None:\n",
    "            episodes.append(ep)\n",
    "        else:\n",
    "            episodes.append(TOTAL_EPISODES)  # Did not converge\n",
    "    conv_data[strategy] = episodes\n",
    "\n",
    "x_pos = np.arange(len(STRATEGIES))\n",
    "means = [np.mean(conv_data[s]) for s in STRATEGIES]\n",
    "stds = [np.std(conv_data[s]) for s in STRATEGIES]\n",
    "bar_colors = [STRATEGY_COLORS[s] for s in STRATEGIES]\n",
    "\n",
    "bars = ax.bar(x_pos, means, yerr=stds, color=bar_colors, alpha=0.7,\n",
    "              capsize=5, edgecolor='black', linewidth=0.5)\n",
    "\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels([STRATEGY_LABELS[s].replace(' (', '\\n(') for s in STRATEGIES],\n",
    "                   fontsize=10)\n",
    "ax.set_ylabel('Episodes to Convergence (50% threshold)')\n",
    "ax.set_title('Sample Efficiency: Convergence Speed')\n",
    "ax.axhline(y=TOTAL_EPISODES, color='gray', linestyle='--', alpha=0.3)\n",
    "\n",
    "for bar, mean_val in zip(bars, means):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 20,\n",
    "            f'{mean_val:.0f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig4_convergence.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Figure 4 saved: fig4_convergence.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f03851",
   "metadata": {},
   "source": [
    "### 6.5 Best Episode 충전 프로필 (Time-Step 분석)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca2352d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_best_episode(agent, env, seed=42):\n",
    "    \"\"\"Run a single episode with best agent, recording full trajectory.\"\"\"\n",
    "    obs, _ = env.reset(seed=seed)\n",
    "    trajectory = {'ergotropy': [], 'energy': [], 'purity': [],\n",
    "                  'entropy': [], 'g': [], 'delta': [], 'rewards': []}\n",
    "    for _ in range(env.max_steps):\n",
    "        action = agent.select_action(obs, deterministic=True)\n",
    "        obs, r, done, trunc, info = env.step(action)\n",
    "        trajectory['ergotropy'].append(info['ergotropy'])\n",
    "        trajectory['energy'].append(info['energy_stored'])\n",
    "        trajectory['purity'].append(info['purity'])\n",
    "        trajectory['entropy'].append(info['entropy'])\n",
    "        trajectory['g'].append(info['g'])\n",
    "        trajectory['delta'].append(info['delta'])\n",
    "        trajectory['rewards'].append(r)\n",
    "        if done or trunc:\n",
    "            break\n",
    "    return trajectory\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "eval_env = DickeOQBEnv()\n",
    "\n",
    "for strategy in STRATEGIES:\n",
    "    # Use best seed (highest final ergotropy)\n",
    "    best_idx = np.argmax([np.mean(r['episode_ergotropy'][-100:])\n",
    "                          for r in all_results[strategy]])\n",
    "    agent = all_results[strategy][best_idx]['agent']\n",
    "    traj = run_best_episode(agent, eval_env)\n",
    "\n",
    "    t = np.arange(len(traj['ergotropy']))\n",
    "    color = STRATEGY_COLORS[strategy]\n",
    "    label = STRATEGY_LABELS[strategy]\n",
    "\n",
    "    axes[0, 0].plot(t, traj['ergotropy'], color=color, label=label, linewidth=2)\n",
    "    axes[0, 1].plot(t, traj['purity'], color=color, label=label, linewidth=2)\n",
    "    axes[1, 0].plot(t, traj['g'], color=color, label=label, linewidth=1.5, alpha=0.8)\n",
    "    axes[1, 1].plot(t, traj['delta'], color=color, label=label, linewidth=1.5, alpha=0.8)\n",
    "\n",
    "axes[0, 0].set_title('Ergotropy vs Time Step'); axes[0, 0].set_ylabel('Ergotropy')\n",
    "axes[0, 1].set_title('Purity vs Time Step'); axes[0, 1].set_ylabel('Purity')\n",
    "axes[1, 0].set_title('Coupling g(t)'); axes[1, 0].set_ylabel('g(t)')\n",
    "axes[1, 1].set_title('Detuning Δ(t)'); axes[1, 1].set_ylabel('Δ(t)')\n",
    "\n",
    "for ax in axes.flat:\n",
    "    ax.set_xlabel('Time Step')\n",
    "    ax.legend(fontsize=8)\n",
    "\n",
    "plt.suptitle('Best Episode Charging Profile', fontsize=14, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.savefig('fig5_charging_profile.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Figure 5 saved: fig5_charging_profile.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff45645",
   "metadata": {},
   "source": [
    "## 7. Statistical Analysis & Discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361800ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" STATISTICAL ANALYSIS: Curriculum Strategy Comparison\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Final performance (last 100 episodes average)\n",
    "print(\"\\n### Final Performance (last 100 episodes average) ###\\n\")\n",
    "perf_data = {}\n",
    "for strategy in STRATEGIES:\n",
    "    vals = [np.mean(r['episode_ergotropy'][-100:]) for r in all_results[strategy]]\n",
    "    perf_data[strategy] = vals\n",
    "    print(f\"  {STRATEGY_LABELS[strategy]:40s}: \"\n",
    "          f\"{np.mean(vals):.4f} ± {np.std(vals):.4f}\")\n",
    "\n",
    "# Pairwise t-tests (Ours vs. others)\n",
    "print(\"\\n### Pairwise Welch's t-test: Ours vs. Others ###\\n\")\n",
    "ours_vals = perf_data['Ours']\n",
    "for strategy in ['PosCurr', 'Vanilla', 'RevCurr']:\n",
    "    other_vals = perf_data[strategy]\n",
    "    t_stat, p_val = stats.ttest_ind(ours_vals, other_vals, equal_var=False)\n",
    "    sig = \"***\" if p_val < 0.01 else (\"**\" if p_val < 0.05 else (\"*\" if p_val < 0.1 else \"n.s.\"))\n",
    "    print(f\"  Ours vs {STRATEGY_LABELS[strategy]:35s}: \"\n",
    "          f\"t={t_stat:+.3f}, p={p_val:.4f} {sig}\")\n",
    "\n",
    "# Convergence speed\n",
    "print(\"\\n### Convergence Speed (episodes to reach 50% max ergotropy) ###\\n\")\n",
    "for strategy in STRATEGIES:\n",
    "    eps = []\n",
    "    for r in all_results[strategy]:\n",
    "        e = r['convergence_episode']\n",
    "        eps.append(e if e is not None else TOTAL_EPISODES)\n",
    "    print(f\"  {STRATEGY_LABELS[strategy]:40s}: \"\n",
    "          f\"{np.mean(eps):.0f} ± {np.std(eps):.0f} episodes\")\n",
    "\n",
    "# Purity maintenance\n",
    "print(\"\\n### Final Purity (last 100 episodes average) ###\\n\")\n",
    "for strategy in STRATEGIES:\n",
    "    vals = [np.mean(r['episode_purity'][-100:]) for r in all_results[strategy]]\n",
    "    print(f\"  {STRATEGY_LABELS[strategy]:40s}: \"\n",
    "          f\"{np.mean(vals):.4f} ± {np.std(vals):.4f}\")\n",
    "\n",
    "# Summary table\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" SUMMARY TABLE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\n{'Method':<35} {'Ergotropy':>12} {'Conv. Ep':>12} {'Purity':>12}\")\n",
    "print(\"-\" * 71)\n",
    "for strategy in STRATEGIES:\n",
    "    erg = np.mean([np.mean(r['episode_ergotropy'][-100:]) for r in all_results[strategy]])\n",
    "    conv_eps = []\n",
    "    for r in all_results[strategy]:\n",
    "        e = r['convergence_episode']\n",
    "        conv_eps.append(e if e is not None else TOTAL_EPISODES)\n",
    "    conv = np.mean(conv_eps)\n",
    "    pur = np.mean([np.mean(r['episode_purity'][-100:]) for r in all_results[strategy]])\n",
    "    print(f\"{STRATEGY_LABELS[strategy]:<35} {erg:>12.4f} {conv:>12.0f} {pur:>12.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e962edcd",
   "metadata": {},
   "source": [
    "## 8. 결론 및 논의\n",
    "\n",
    "### 실험 결과 해석\n",
    "\n",
    "위 결과는 동일 조건(에이전트, 하이퍼파라미터, 총 학습량)에서 4가지 커리큘럼 전략을\n",
    "객관적으로 비교한 것입니다.\n",
    "\n",
    "**평가 기준:**\n",
    "1. **최종 에르고트로피**: 충전 성능의 절대적 수준\n",
    "2. **수렴 속도**: 목표 달성까지의 샘플 효율성\n",
    "3. **순도 유지**: 양자 코히어런스 보존 능력\n",
    "4. **안정성**: seed 간 분산 (robust한가?)\n",
    "\n",
    "### 한계점\n",
    "- N=4 qubits의 소규모 시스템으로 제한적\n",
    "- Dicke manifold 근사 사용 (개별 qubit 역학 무시)\n",
    "- 학습 에피소드 수가 제한적 (더 긴 학습 시 결과가 달라질 수 있음)\n",
    "- 단일 OQB 모델(Dicke)만 테스트 — TC, Spin Chain 등 다른 모델에서의 일반화 필요\n",
    "\n",
    "### 향후 연구 방향\n",
    "- 더 큰 N (8, 16 qubits)으로 확장\n",
    "- 다른 양자 배터리 모델(TC, Spin Chain)에서 검증\n",
    "- Non-Markovian 환경에서의 비교\n",
    "- Negative Curriculum의 스테이지 전환 조건 자동화 연구\n",
    "\n",
    "---\n",
    "*이 노트북은 bias를 최소화하기 위해 모든 방법에 동일한 리소스를 할당하고,\n",
    "통계적 유의성 검증을 포함합니다.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0b896c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\" Experiment Complete!\")\n",
    "print(\" All figures saved to current directory.\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
